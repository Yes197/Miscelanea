qnorm(0.8485)
qnorm(2.82842)
qnorm(2.334)
qnorm(1-2.334)
qnorm(2.334-1)
2*pnorm(-abs(-2.6068))
2*pnorm(-2.6068)
2*pnorm(2.6068)
2*pnorm(2.6068, lower.tail = FALSE)
2*pnorm(-2.6068)/2
2*pnorm(-2.6068)/2
pnorm(-2.6068)
pnorm(1.32)
1-pnorm(-2.6068)
1-pnorm(1.32)
pnorm(-1.7422)
2*pnorm(-1.7422)
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(corrplot)
library(xgboost)
library(cowplot)
setwd('C:/Users/Asus/Mis Jupyter/Miscelanea/Big Smart problem')
train = fread("train_v9rqX0R.csv")
test = fread("test_AbJTz2l.csv")
submission = fread("sample_submission_8RXa3c6.csv")
dim(train)
dim(test)
names(train)
names(test)
str(train)
glimpse(train)
str(test)
test[, Item_Outlet_Sales := NA]
combi = rbind(train, test)  # combining test and train
dim(combi)
head(combi)
# 1. UNIVARIATE ANALYSIS
ggplot(train) + geom_histogram(aes(train$Item_Outlet_Sales), binwidth = 100, fill = "darkgreen") + xlab("Item_Outlet_Sales")
# INDEPENDENT VARIABLES - Numeric
# We use histogram because that will help in visualizing the variables distribution
p1 = ggplot(combi) + geom_histogram(aes(Item_Weight), binwidth = 0.5, fill = "blue")
p2 = ggplot(combi) + geom_histogram(aes(Item_Visibility), binwidth = 0.005, fill = "blue")
p3 = ggplot(combi) + geom_histogram(aes(Item_MRP), binwidth = 1, fill = "blue")
plot_grid(p1, p2, p3, ncol = 1)
# observations:
# There is no clear-cut pattern in Item_Weight
# Item_Visibility is right-skewed should be transformed to curb its skewness
# We clearly see 4 distributions in Item_MRP
# INDEPENDENT VARIABLES - Categorical
# Categorical features can have only a finite set of values
ggplot(combi %>% group_by(Item_Fat_Content) %>% summarise(Count = n())) + geom_col(aes(Item_Fat_Content, Count), fill = "coral1")
# also we can use geom_bar(aes(Item_Fat_Content, Count), stat = "identity", fill = "coral1")
# observations:
# LF, low fat is Low Fat similarly reg with Regular, we should combine it
combi$Item_Fat_Content[combi$Item_Fat_Content == "LF"] = "Low Fat"
combi$Item_Fat_Content[combi$Item_Fat_Content == "low fat"] = "Low Fat"
combi$Item_Fat_Content[combi$Item_Fat_Content == "reg"] = "Regular"
ggplot(combi %>% group_by(Item_Fat_Content) %>% summarise(Count = n())) + geom_col(aes(Item_Fat_Content, Count), fill = "coral1")
# OTHER CATEGORICAL VARIABLES
p4 = ggplot(combi %>% group_by(Item_Type) %>% summarise(Count = n())) + geom_col(aes(Item_Type, Count), fill = "coral1") + xlab("") + geom_label(aes(Item_Type, Count, label = Count), vjust = 0.5) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle("Item_Type")
p5 = ggplot(combi %>% group_by(Outlet_Identifier) %>% summarise(Count = n())) + geom_col(aes(Outlet_Identifier, Count), fill = "coral1") + xlab("") + geom_label(aes(Outlet_Identifier, Count, label = Count), vjust = 0.5) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
p6 = ggplot(combi %>% group_by(Outlet_Size) %>% summarise(Count = n())) + geom_col(aes(Outlet_Size, Count), fill = "coral1") + xlab("") + geom_label(aes(Outlet_Size, Count, label = Count), vjust = 0.5) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
second_row = plot_grid(p5, p6, nrow = 1)
plot_grid(p4, second_row, ncol = 1)
# observations:
# There are 4016 blanks in Outlet_Size we will check bivariate analysis to substitute them
# REST OF THE CATEGORICAL VARIABLES
# We need to transform Outlet_Establishment_Year from int to categorical variable
combi$Outlet_Establishment_Year = as.factor(combi$Outlet_Establishment_Year)
p7 = ggplot(combi %>% group_by(Outlet_Establishment_Year) %>% summarise(Count = n())) + geom_col(aes(Outlet_Establishment_Year, Count), fill = "coral1") + xlab("") + geom_label(aes(Outlet_Establishment_Year, Count, label = Count), vjust = 0.5) + xlab("Outlet_Establishment_Year") + theme(axis.text.x = element_text(size = 8.5))
p8 = ggplot(combi %>% group_by(Outlet_Type) %>% summarise(Count = n())) + geom_col(aes(Outlet_Type, Count), fill = "coral1") + xlab("") + geom_label(aes(Outlet_Type, Count, label = Count), vjust = 0.5) + theme(axis.text.x = element_text(size = 8.5))
plot_grid(p7, p8, ncol = 2)
# observations:
# Lesser number of observations in 1998 category in Outlet_Establishment_Year
# Supermarket type 1 seems to be more popular than the others types of Outlet_Type
# 2. BIVARIATE ANALYSIS
train = combi[1:nrow(train)] # extract train dataset
# TARGET VARIABLE VS NUMERICAL VARIABLES
# scatter plot for numeric
p9 = ggplot(train) + geom_point(aes(Item_Weight, Item_Outlet_Sales), colour = "blue", alpha = 0.3) + theme(axis.title = element_text(size = 8.5))
p10 = ggplot(train) + geom_point(aes(Item_Visibility, Item_Outlet_Sales), colour = "blue", alpha = 0.3) + theme(axis.title = element_text(size = 8.5))
p11 = ggplot(train) + geom_point(aes(Item_MRP, Item_Outlet_Sales), colour = "blue", alpha = 0.3) + theme(axis.title = element_text(size = 8.5))
second_row_2 = plot_grid(p10, p11, ncol = 2)
plot_grid(p9, second_row_2, nrow = 2)
# observations:
# Item_Outlet_Sales is spread well across the range of Item_Weight
# Item_Visibility vs Item_Outlet_Sales there is a string at Item_Visibility = 0.0 It's strange since Item_Visibility cannot be 0. We will deal with this issue later
# Item_MRP vs Item_Outlet_Sales we can see 4 segments of prices that can be used in feature engineering to create a new variable
# TARGET VARIABLE VS INDEPENDENT CATEGORICAL VARIABLES
# violin plot or boxplot for categorical, we use violin as they show the full distribution of the data
# the width of a violin indicates concentration or density of the data
# the height of a violin tells about the range of the target variable values
p12 = ggplot(train) + geom_violin(aes(Item_Type, Item_Outlet_Sales), fill = "magenta") + theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text = element_text(size = 6), axis.title = element_text(size = 8.5))
p13 = ggplot(train) + geom_violin(aes(Item_Fat_Content, Item_Outlet_Sales), fill = "magenta") + theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text = element_text(size = 6), axis.title = element_text(size = 8.5))
p14 = ggplot(train) + geom_violin(aes(Outlet_Identifier, Item_Outlet_Sales), fill = "magenta") + theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text = element_text(size = 6), axis.title = element_text(size = 8.5))
second_row_3 = plot_grid(p13, p14, ncol = 2)
plot_grid(p12, second_row_3, nrow = 2)
# observations:
# Distribution of Item_Outlet_Sales across the categories of Item_Type is not very distinct same with Item_Fat_Content
# Distribution of OUT010, OUT019 categories are quite similar and different from the rest of the Outlet_Identifier
# CHECK Outlet_Size vs Item_Outlet_Sales
# We check the distribution of the target variable across Outlet_Size
ggplot(train) + geom_violin(aes(Outlet_Size, Item_Outlet_Sales), fill = "magenta")
# observations:
# Small category distribution of Outlet_Size is almost equal to the blank category (we could sustite blank with Small) it is not the only way, but we will do this
# REST OF THE CATEGORICAL VARIABLES
p15 = ggplot(train) + geom_violin(aes(Outlet_Location_Type, Item_Outlet_Sales), fill = "magenta")
p16 = ggplot(train) + geom_violin(aes(Outlet_Type, Item_Outlet_Sales), fill = "magenta")
plot_grid(p15, p16, ncol = 1)
# observations:
# Tier 1 and 3 of Outlet_Location_Type look similar
# In Outlet_Type the Grocery store category has more of its data around lower Item_Outlet_Sales values compared to the other categories
# 3. MISSING VALUE TREATMENT
# Deletion of rows: in train dataset missing values are deleted, the downside is the loss of information and drop in prediction power of the model
# Mean/Median/Mode imputation: for continuous variables it is replace for those values applied to the existing data. For categorical we can use mode.
# Building prediction model: we can make a predictive model to impute missing values. The missing value is the target variable and the other variables as predictors. We divide our data into 2 datasets. One without missing values and the other with the missing values for that variable. The former set will be used as a training to build the predictive model and it would then be applied to the latter set to predict missing values.
# code to easily find the missing values in a variable
sum(is.na(combi$Item_Weight))
# IMPUTING MISSING VALUES
# We have missing values in Item_Outlet_Sales and Item_Weight. We can ignore missing data in Item_Outlet_Sales since they belong to the test dataset.
# Impute Item_Weight with mean based on the Item_Identifier variable
missing_index = which(is.na(combi$Item_Weight))
for(i in missing_index) {
item = combi$Item_Identifier[i]
combi$Item_Weight[i] = mean(combi$Item_Weight[combi$Item_Identifier == item], na.rm = T)
}
sum(is.na(combi$Item_Weight))
# REPLACING 0s
# similarly, we can replace the 0s in Item_Visibility also with Item_Identifier wise mean. We can visualize the zeros in the following plot
ggplot(combi) + geom_histogram(aes(Item_Visibility), bins = 100)
zero_index = which(combi$Item_Visibility == 0)
for (i in zero_index) {
item = combi$Item_Identifier[i]
combi$Item_Visibility[i] = mean(combi$Item_Visibility[combi$Item_Identifier == item], na.rm = T)
}
# we plot to see if all the values were replaced
ggplot(combi) + geom_histogram(aes(Item_Visibility), bins = 100)
# 4. FEATURE ENGINEERING
# When the features given in a dataset are no sufficient to give satisfactory predictions, we have to create new ones to help to improve our model's performance. We will create:
# Item_Type_new: Broader categories for the variable Item_Type
# Item_category: Categorical variable derived from Item_Identifier
# Outlet_Years: Years of operation for outlets
# price_per_unit_wt: Item_MRP/Item_Weight
# Item_MRP_clusters: Binned feature for Item_MRP
# We can look at Item_Type and classify the categories into perishable and non_perishable, as per our understanding, and make it into a new feature
perishable = c("Breads", "Breakfast", "Dairy", "Fruits and Vegetables", "Meat", "Seafood")
non_perishable = c("Soft Drinks", "Household", "Baking Goods", "Frozen Foods", "Health and Hygiene", "Hard Drinks", "Canned")
# Create a new feature "Item_Type_new"
combi[, Item_Type_new := ifelse(Item_Type %in% perishable, "perishable", ifelse(Item_Type %in% non_perishable, "non_perishable", "not_sure"))]
# We compare Item_Type with the first 2 characters of Item_Identifier, they stand for drink, food and non-consumable
table(combi$Item_Type, substr(combi$Item_Identifier, 1, 2))
# Based on that table we can create a new feature, Item_category
combi[, Item_category := substr(combi$Item_Identifier, 1, 2)]
# Change the value of Item_Fat_Content wherever Item_category is NC they cannot have any Fat Content
combi$Item_Fat_Content[combi$Item_category == "NC"] = "Non-Edible"
# Years operation
combi[, Outlet_Years := 2013 - as.integer(Outlet_Establishment_Year)]
# Price per unit weight
combi[, price_per_unit_wt := Item_MRP/Item_Weight]
# Earlier in Item_MRP vs Item_Outlet_Sales we saw Item_MRP spread in 4 chunks. Now we will assign a label to each chunk and use this label as a new variable
combi[, Item_MRP_clusters := ifelse(Item_MRP < 69, "1st", ifelse(Item_MRP >= 69 & Item_MRP < 136, "2nd", ifelse(Item_MRP >= 136 & Item_MRP < 203, "3rd", "4th")))]
# 5. ENCODING CATEGORICAL VARIABLES
# ML Models produce better result with numerical variables. For that reason we will convert our categorical variables to numerical ones. We will use 2 techniques
# Label encoding: means converting each category of a variable to a number. It is more suitable for ordinal variables
# One hot encoding: each category of categorical variable is converted into a new binary column
# LABEL ENCODING
# Outlet_Size and Outlet_Location_Type
combi[, Outlet_Size_num := ifelse(Outlet_Size == "Small", 0, ifelse(Outlet_Size == "Medium", 1, 2))]
combi[, Outlet_Location_Type_num := ifelse(Outlet_Location_Type == "Tier 3", 0, ifelse(Outlet_Location_Type == "Tier 2", 1, 2))]
# Removing categorical variables after label encoding
combi[, c("Outlet_Size", "Outlet_Location_Type") := NULL]
# ONE HOT ENCODING
ohe = dummyVars("~.", data = combi[, -c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")], fullRank = T)
ohe_df = data.table(predict(ohe, combi[, -c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")]))
combi = cbind(combi[, "Item_Identifier"], ohe_df)
# 6. DATA PREPROCESSING
# PreProcessing refers to the transformations applied to your data before feeding it to the algorithm. It involves further data cleaning, data transformation, data scaling and more things.
# Here we will deal with skewness and scale the numerical values
# REMOVING SKEWNESS
# Item_Visibility, price_per_unit_wt are highly skewed, we will remove with log transformation
combi[, Item_Visibility := log(Item_Visibility + 1)]  # log + 1 to avoid division by zero
combi[, price_per_unit_wt := log(price_per_unit_wt + 1)]
# SCALING NUMERIC PREDICTORS
# Scale and center the numeric variables to make them have zero mean, stdev 1 and scale from 0 to 1. This is necessary for linear regression models
num_vars = which(sapply(combi, is.numeric))  # index of numeric features
num_vars_names = names(num_vars)
combi_numeric = combi[, setdiff(num_vars_names, "Item_Outlet_Sales"), with = F]
prep_num = preProcess(combi_numeric, method = c("center", "scale"))
combi_numeric_norm = predict(prep_num, combi_numeric)
combi[, setdiff(num_vars_names, "Item_Outlet_Sales") := NULL]  # removing numeric independent variables
combi = cbind(combi, combi_numeric_norm)
# Split again into test and train data
train = combi[1:nrow(train)]
test = combi[(nrow(train) +1):nrow(combi)]
test[, Item_Outlet_Sales := NULL]  # removing Item_Outlet_Sales as it only contains NA for test data
# CORRELATED VARIABLES
# We will examine correlated features of train dataset. It is not desirable to have correlated features if we are using linear regressions
cor_train = cor(train[, -c("Item_Identifier")])
corrplot(cor_train, method = "pie", type = "lower", tl.cex = 0.4)
# The plot between any two variables is represented by a pie. Blueish pie indicates positive correlation and reddish one represent negative correlation. The correlation magnitude is denoted by the area covered by the pie.
# observations:
# Variables price_per_unit_wt and Item_Weight are highly positive correlated as the first one was created from the second one. The same happens with price_per_unit_wt and Item_MRP
# 7. LINEAR REGRESSION
# There are assumptions data need to satisfy to be a good regression model:
# Absence of multicollinearity, in other words, independent variables should be correlated. This issue can be dealt with regularization
# As per the plot above we have few highly correlated independent variables
# EVALUATION METRICS FOR LINEAR REGRESSION
# MAE: mean absolute error
# (1/n)*sum(y_i - y0_i)
# MSE: mean squared error
# (1/n)*sum(y_i - y0_i)^2
# RMSE: root mean squared error
# sqrt((1/n)*sum(y_i - y0_i)^2)
# We will use 5-fold cross validation in all the models, in order to the model generalizes unseen data
linear_reg_mod = lm(Item_Outlet_Sales ~ ., data = train[, -c("Item_Identifier")])
# Making prediction on test Data
submission$Item_Outlet_Sales = predict(linear_reg_mod, test[, -c("Item_Identifier")])
write.csv(submission, "Linear_Reg_submit.csv", row.names = F)
# 8. REGULARIZED LINEAR REGRESSION
# This model can handle the correlated independent variables very well and help overcoming overfiting.
# Ridge: penalty shrinks the coefficients of correlated predictors towards each other
# Lasso: tends to pick one of a pair of correlated features and discard the other
# The tuning parameter lambda controls the strength of the penalty
# LASSO REGRESSION
set.seed(1235)
my_control = trainControl(method = "cv", number = 5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001, 0.1, by = 0.002))
lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales, method = "glmnet", trControl = my_control, tunedGrid = Grid)
my_control = trainControl(method = "cv", number = 5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001, 0.1, by = 0.002))
lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales, method = "glmnet", trControl = my_control, tunedGrid = Grid)
# RIDGE REGRESSION
set.seed(1236)
my_control = trainControl(method = "cv", number = 5)
my_control = trainControl(method = "cv", number = 5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001, 0.1, by = 0.0002))
lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales, method = "glmnet", trControl = my_control, tunedGrid = Grid)
# RIDGE REGRESSION
set.seed(1236)
my_control = trainControl(method = "cv", number = 5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001, 0.1, by = 0.0002))
ridge_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales, method = "glmnet", trControl = my_control, tunedGrid = Grid)
# 9. RANDOM FOREST
# We will build a random forest with 400 trees
# mtry  = # of predictor variables randomly sampled at each split
# min.node.size = minimun size of terminal nodes (setting this number large causes smaller trees and reduces overfitting)
set.seed(1237)
my_control = trainControl(method = "cv", number = 5)  # 5-fold CV
tgrid = expand.grid(
.mtry = c(3:10),
.splitrule = "variance",
.min.node.size = c(10, 15, 20)
)
rf_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales, method = "ranger", trControl = my_control, tuneGrid = tgrid, num.trees = 400, importance = "permutation")
# lets visualize RMSE scores for different tuning parameters
plot(rf_mod)
# lets plot feature importance based on the RandomForest model
plot(varImp(rf_mod))
param_list = list(objective = "reg:linear", eta = 0.01, gamma = 1, max_depth = 6, subsample = 0.8, colsample_bytree = 0.5)
dtrain = xgb.DMatrix(data = as.matrix(train[, -c("Item_Identifier", "Item_Outlet_Sales")]), label = train$Item_Outlet_Sales)
dtest = xgb.DMatrix(data = as.matrix(test[, -c("Item_Identifier", "Item_Outlet_Sales")]))
# Cross Validation
# xgb.cv() for cross validation
set.seed(112)
xgbcv = xgb.cv(params = param_list, data = dtrain, nrounds = 1000, nfold = 5, print_every_n = 10, early_stopping_rounds = 30, maximize = F)
# Model training
xgb_model = xgb.train(data = dtrain, params = param_list, nrounds = 430)
# Variable importance
var_imp = xgb.importance(feature_names = setdiff(names(train), c("Item_Identifier", "Item_Outlet_Sales")), model = xgb_model)
xgb.plot.importance(var_imp)
